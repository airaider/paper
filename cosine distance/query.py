주제 1 : 인터넷상의 영화 리뷰 크롤링(홍기환, 남정원)

 - 네이버에 올라오는 영화 리뷰들은 quality 보장이 어려움. 전문가가 아닌 글들

 - 따라서 영화 전문 사이트들을 조사하고, 이들을 인지도, 방문자 수 등..(어떤 factor로 영화 전문 사이트들 우선순위 서열을 결정할지도 분석)

 - 이들 사이트들과 크롤링 소프트웨어 개발

 - 각각 사이트들과 크롤링하는 모듈들을 개발

 - (필요시) 크롤링 동합 플랫폼 패키지 개발도 가능 여부 파악(?)

               . 각 사이트별 크롤링 소프트웨어 개발시 어떤 것들을 공통 모듈로 할 것인가 분석 및 결정

               . 에러/예외 처리

               . DB 활용 혹은 리뷰 검색

    

 - 영화에 대한 리뷰 확보



주제 2 : 리뷰 감성 분석 및 추천

 - 먼저 영화에 대한 다양한 리뷰들이 어떤 형식, 내용으로 구성되는지 분석

 - 이 리뷰들의 키워드 추출

 - 키워드들을 수집하여 DB화, 자연어 처리 기술 등

 - (필요시) 딥러닝 기반의 알고리즘 적용

 - 고객들이 영화들을 판단할 때 어떤 방법으로 질문, 영화를 선택하는 방식 분석

   . "~~~ " 영화 추천해 줘(~~~ : 재밌는? 싸우는? 애정?.......

  - 이런 질문에 대응하여 답을 제공하기 위해 ? 키워드들을 어떻게 확보 관리 해야 할지"

첫째 주는 사전 조사의 단계를 진행하였다. 전체적인 프로젝트의 방향 뿐만 아니라 1월7일 OT 자료 준비, 톨게이트 사전 신청, 주간 보고서 작성 등의 작업을 진행하였다. 알티캐스트 같은 경우에는 머신러닝 한팀, 크롤링 한팀으로 팀이 나뉘어졌으며 각 팀의 리더는 이영건, 홍기환 학생이, 전체 팀장은 이영건 학생이 맡기로 결정을 하였다. 5주 동안의 진척 상황 및 예상 스케쥴을 작성하여 교수님한테 이메일로 전송을 하였으며 피드백을 받았다. 프로젝트를 진행하는 동안 작성해야할 보고서 및 서류들이 많기 때문에 톨게이트 시스템에 인적사항을 기입을 하였으며 해당 기업에 신청을 진행하였다. 이러한 사전 작업 뿐만 아니라 팀 내부적으로도 발표 자료를 작성하였다.  앞으로 세부적으로 선정해야할 것들은 크롤링을 어떠한 사이트에서 진행하게 될건지, 크롤링할 내용이 어떠한 것들인지, 어떠한 데이터 포맷으로 저장을 하게 될건지, 어떠한 오픈소스를 활용할지에 대한 추가적인 상의가 필요하다. 5주 계획서 상으로는 2~3주까지는 크롤링 작업을 마무리하고 추가적인 소프트웨어 및 플랫폼을 통해 간편하게 크롤링을 진행할 수 있도록 개발을 해야한다. 머신러닝팀이 크롤링팀의 데이터를 활용하여 진행을 해야하기 때문에 프로젝트 하반기에는 두 팀 모두 같이 작업을 진행해야할 것으로 보인다. 마지막 주에는 두 팀이 합동으로 제작한 소프트웨어를 데모 및 평가를 받아야하기 때문에 각 팀별로 최대한 작업을 빨리 마무리 지어서 먼저 마무리하는 팀대로 상대 팀이랑 같이 협업을 진행하는 것이 효율성을 높일것으로 보인다. 1월 7일부터 본격적으로 시작하는 산합협력프로젝트인 만큼 많은 기대를 안고 시작하게 될것으로 보인다.
2주차에는 기업과 화상회의를 진행하면서 전체적인 과제의 로드맵을 재차 확인하는 시간을 가졌다. Contents Based 기반으로 영화를 추천하는데, 이번 프로젝트에서 맡은 역할은 전문성 있는 리뷰를 이 추천 알고리즘의 feature로 사용해보는 것/query에 리뷰 데이터를 활용하여 응답하는 것이 목표이다. 개발시에 유의해야할 사항은 유사어 사전 오픈 소스를 찾아보고 필요시 만들고, 쉽게 수정할 수 있어야 할 것으로 보인다. 또한 동일 제목 영화를 피하기 위해 년도를 포함한 데이터 set이 추가적으로 필요할 것으로 보여진다. 그리고 크롤링팀 내부적으로 진행사항을 검토하고 리뷰해서 정리를 하였다. 첫 번째 상의한 내용은 어떠한 데이터를 긁어올지에 대한 거였다. 양질의 리뷰 데이터가 필요한거였기 때문에 유저기반의 리뷰보다는 전문가 위주의 리뷰를 긁어오기로 했다. 영화 제목 및 개봉 연도 그리고 추가적으로 영화의 장르를 추가적으로 크롤링이 필요했다. 시놉시스가 필요하면 추가적으로 붙이기로 했다. 어떠한 사이트를 조사할지에 대해서는 씨네21을 선정하였다. 크롤링은 뷰티풀수프이라는 라이브러리를 활용하여 html에서 클래스를 긁어오고 텍스트로 변환하여 변수에 담아서 전송하는 형식으로 진행하였다. 향후 개발 일정으로는 csv파일을 머신러닝팀과 상의하여 팀에서 사용할 모델에 알맞은 입력 형식으로 변환할 필요가 있다. 일단 주어진 데이터 set으로 머신러닝팀과 함께 시험적으로 모델을 돌려보고 output을 확인해야 하는 일이 향후 남은 시간동안 해야할 과제이다.
3주차에는 기업 방문을 통해 현재까지의 진행상황을 보고를 했다. 크롤링 팀에서 국내외 전문가 영화 리뷰사이트에 대해서 조사를 했지만, 아무래도 기업측에서 원하는 것은 한국어로 된 영화 추천 시스템이다 보니 문제가 있었다. 그 이유는 일단 한국 고객들을 대상으로 하기 때문에, 한국어 리뷰들을 기반으로 해야 하고, 이미 형태소 분석이나 모델을 돌릴 때 에러가 많이 발생하는데, Query 단계에서 영어번역이 들어가면 에러가 누적이 되어 결과적으로 성능이 저하된다는 문제점을 지적해주셨습니다. 국내의 전문가 리뷰 사이트 위주로 조사를 진행해야 한다고 말씀하셨다. 하지만, 국내 전문가 리뷰 사이트는 씨네 21을 제외하고 길이가 너무 짧다는 문제점이 있었다. 따라서 해당 문제를 해결하기 위해서는 다양한 사이트에서 길이가 비교적 긴 리뷰들을 위주로 크롤링을 진행해야 했다. IMDB에서 긁어온 데이터는 폐기 처리를 해야했다. 한편, 추천팀에서는 영문 자연어 처리 모듈인 NLTK와 한국어 자연어 처리 모듈인 KoNLPy를 학습하고, 직접 리뷰사이트에서 복사한 데이터를 가지고 실습을 진행해봤습니다. 또한, 자동으로 Feature를 찾아주는 Embedding기법인 Word2Vec으로 모델도 학습시켜 보았는데, 한 클러스터에 반대어 같이 묶어서 나오는 문제와 역시나 학습데이터 부족하여서 성능이 떨어진다는 문제점이 있었습니다. 각 팀으로 문제점을 종합적으로 분석해봤을 때, 저희는 해결방향을 영어로 접근해보는 것으로 잡았고, 그 이유는 일단 영문으로 된 영화리뷰가 많고, 영어기반 자연어처리 툴이 한국어보다 성능이 뛰어났기 때문입니다. 
4주차에는 중간 발표 피드백을 통해서 다양한 사이트들에서 크롤링 데이터를 충분히 확보를 하였다. 다음 영화, 무비스트, 씨네21, 칼럼니스트 블로그 등을 통해 7천개 가량의 데이터를 모았다. 리뷰글에 해당하는 기사, 칼럼, 게시글에서 텍스트에 해당하는 내용만 긁어와서 일정한 포맷으로 영화별로 텍스트 파일에 저장을 한 다음에 추천 시스템 팀에게 제공을 하였다. 각 영화별로 조금씩 제목이 다르고 연도도 일정하지 않아서 취합과정에서 어려움이 있었다. 추천 시스템 팀에서는 크게 word2vec과 tf-idf방식으로 진행을 하고 있었으며 크롤링을 완료한 현 시점에서 이쪽 팀에 붙어서 유의어 사전 구축을 도왔다. 크게 나뉘어져있는 feature 별로 해당 feature에 해당하는 형태소 분석을 완료한 유의어들의 사전을 만들었다. 유의어 사전의 예시로는 horror = set(['살점', '끔찍하', '찌푸리', '질색', '죽이려','공포','무시무시','섬뜩하','불안', '경악', '두려', '긴장', '숨통', '호러'])등이 있다. feature가 많을수록 그리고 유의어 또한 다양할수록 전체적인 시스템의 성능이 향상될 것으로 보이며 쿼리문을 통해 들어오는 스트링 값이 길고 단어가 다양할수록 높은 정확도를 리턴 받을 수 있을 것으로 예상된다. 내부 회의를 통해 향후 프로젝트의 진행 방향을 다시 잡는 시간을 가지기도 했다. 앞으로 진행해야할 사항들에는 word2vec을 활용하여 wiki data로 학습하여 유의어 사전 구축을 진행해야하며 feature들 간의 거리 계산 결과를 csv파일로 저장을 해야한다. csv로 구축을 하는 이유는 속도의 향상을 위해서이며 pandas를 활용해야한다. 또한 쿼리문으로 들어오는 스트링을 형태소분석을 통해 feature화를 하여 feature 유의어 사전에 활용할 예정이다. query의 진행속도를 향상시키기 위해서 모든 영화의 feature vector에 대해 cosine distance을 구할순 없기 때문에 feature당 높은 값을 기준으로 상위 몇 퍼센트만 처리한다든지 같은 방식을 활용해야 할 것이다.